/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.flink.table.functions.hive;

import org.apache.flink.annotation.Internal;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.typeutils.GenericTypeInfo;
import org.apache.flink.table.catalog.hive.client.HiveShim;
import org.apache.flink.table.catalog.hive.util.HiveTypeUtil;
import org.apache.flink.table.functions.AggregateFunction;
import org.apache.flink.table.functions.FunctionContext;
import org.apache.flink.table.functions.hive.conversion.HiveInspectors;
import org.apache.flink.table.functions.hive.conversion.HiveObjectConversion;
import org.apache.flink.table.functions.hive.conversion.IdentityConversion;
import org.apache.flink.table.types.DataType;
import org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter;

import org.apache.hadoop.hive.ql.exec.UDAF;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.parse.SemanticException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver2;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;

import java.util.Arrays;

/**
 * An {@link AggregateFunction} implementation that calls Hive's {@link UDAF} or {@link GenericUDAFEvaluator}.
 */
@Internal
public class HiveGenericUDAF
	extends AggregateFunction<Object, GenericUDAFEvaluator.AggregationBuffer> implements HiveFunction {

	private final HiveFunctionWrapper hiveFunctionWrapper;
	// Flag that indicates whether a bridge between GenericUDAF and UDAF is required.
	// Old UDAF can be used with the GenericUDAF infrastructure through bridging.
	private final boolean isUDAFBridgeRequired;

	private Object[] constantArguments;
	private DataType[] argTypes;

	private transient GenericUDAFEvaluator partialEvaluator;
	private transient GenericUDAFEvaluator finalEvaluator;
	private transient ObjectInspector partialResultObjectInspector;
	private transient ObjectInspector finalResultObjectInspector;
	private transient HiveObjectConversion[] conversions;
	private transient boolean allIdentityConverter;
	private transient boolean initialized;

	private final HiveShim hiveShim;

	public HiveGenericUDAF(HiveFunctionWrapper funcWrapper, HiveShim hiveShim) {
		this(funcWrapper, false, hiveShim);
	}

	public HiveGenericUDAF(HiveFunctionWrapper funcWrapper, boolean isUDAFBridgeRequired, HiveShim hiveShim) {
		this.hiveFunctionWrapper = funcWrapper;
		this.isUDAFBridgeRequired = isUDAFBridgeRequired;
		this.hiveShim = hiveShim;
	}

	@Override
	public void open(FunctionContext context) throws Exception {
		super.open(context);
		init();
	}

	private void init() throws HiveException {
		ObjectInspector[] inputInspectors = HiveInspectors.toInspectors(hiveShim, constantArguments, argTypes);

		// Flink UDAF only supports Hive UDAF's PARTIAL_1 and FINAL mode

		// PARTIAL1: from original data to partial aggregation data:
		// 		iterate() and terminatePartial() will be called.
		this.partialEvaluator = createEvaluator(inputInspectors);
		this.partialResultObjectInspector = partialEvaluator.init(GenericUDAFEvaluator.Mode.PARTIAL1, inputInspectors);

		// FINAL: from partial aggregation to full aggregation:
		// 		merge() and terminate() will be called.
		this.finalEvaluator = createEvaluator(inputInspectors);
		this.finalResultObjectInspector = finalEvaluator.init(
			GenericUDAFEvaluator.Mode.FINAL, new ObjectInspector[]{ partialResultObjectInspector });

		conversions = new HiveObjectConversion[inputInspectors.length];
		for (int i = 0; i < inputInspectors.length; i++) {
			conversions[i] = HiveInspectors.getConversion(inputInspectors[i], argTypes[i].getLogicalType());
		}
		allIdentityConverter = Arrays.stream(conversions)
			.allMatch(conv -> conv instanceof IdentityConversion);

		initialized = true;
	}

	private GenericUDAFEvaluator createEvaluator(ObjectInspector[] inputInspectors) throws SemanticException {
		GenericUDAFResolver2 resolver;

		if (isUDAFBridgeRequired) {
			resolver = new GenericUDAFBridge((UDAF) hiveFunctionWrapper.createFunction());
		} else {
			resolver = (GenericUDAFResolver2) hiveFunctionWrapper.createFunction();
		}

		return resolver.getEvaluator(
			hiveShim.createUDAFParameterInfo(
				inputInspectors,
				// The flag to indicate if the UDAF invocation was from the windowing function call or not.
				// TODO: investigate whether this has impact on Flink streaming job with windows
				Boolean.FALSE,
				// Returns true if the UDAF invocation was qualified with DISTINCT keyword.
				// Note that this is provided for informational purposes only and the function implementation
				// is not expected to ensure the distinct property for the parameter values.
				// That is handled by the framework.
				Boolean.FALSE,
				// Returns true if the UDAF invocation was done via the wildcard syntax FUNCTION(*).
				// Note that this is provided for informational purposes only and the function implementation
				// is not expected to ensure the wildcard handling of the target relation.
				// That is handled by the framework.
				Boolean.FALSE));
	}

	/**
	 * This is invoked without calling open() in Blink, so we need to call init() for getNewAggregationBuffer().
	 * TODO: re-evaluate how this will fit into Flink's new type inference and udf system√ü
	 */
	@Override
	public GenericUDAFEvaluator.AggregationBuffer createAccumulator() {
		try {
			if (!initialized) {
				init();
			}
			return partialEvaluator.getNewAggregationBuffer();
		} catch (Exception e) {
			throw new FlinkHiveUDFException(
				String.format("Failed to create accumulator for %s", hiveFunctionWrapper.getClassName()), e);
		}
	}

	public void accumulate(GenericUDAFEvaluator.AggregationBuffer acc, Object... inputs) throws HiveException {
		if (!allIdentityConverter) {
			for (int i = 0; i < inputs.length; i++) {
				inputs[i] = conversions[i].toHiveObject(inputs[i]);
			}
		}

		partialEvaluator.iterate(acc, inputs);
	}

	public void merge(
			GenericUDAFEvaluator.AggregationBuffer accumulator,
			Iterable<GenericUDAFEvaluator.AggregationBuffer> its) throws HiveException {

		for (GenericUDAFEvaluator.AggregationBuffer buffer : its) {
			finalEvaluator.merge(
				accumulator, partialEvaluator.terminatePartial(buffer));
		}
	}

	@Override
	public Object getValue(GenericUDAFEvaluator.AggregationBuffer accumulator) {
		try {
			return HiveInspectors.toFlinkObject(finalResultObjectInspector, finalEvaluator.terminate(accumulator));
		} catch (HiveException e) {
			throw new FlinkHiveUDFException(
				String.format("Failed to get final result on %s", hiveFunctionWrapper.getClassName()), e);
		}
	}

	@Override
	public void setArgumentTypesAndConstants(Object[] constantArguments, DataType[] argTypes) {
		this.constantArguments = constantArguments;
		this.argTypes = argTypes;
	}

	@Override
	public DataType getHiveResultType(Object[] constantArguments, DataType[] argTypes) {
		try {
			if (!initialized) {
				setArgumentTypesAndConstants(constantArguments, argTypes);
				init();
			}

			return HiveTypeUtil.toFlinkType(finalResultObjectInspector);
		} catch (Exception e) {
			throw new FlinkHiveUDFException(
				String.format("Failed to get Hive result type from %s", hiveFunctionWrapper.getClassName()), e);
		}
	}

	@Override
	public TypeInformation getResultType() {
		return LegacyTypeInfoDataTypeConverter.toLegacyTypeInfo(
			getHiveResultType(this.constantArguments, this.argTypes));
	}

	@Override
	public TypeInformation<GenericUDAFEvaluator.AggregationBuffer> getAccumulatorType() {
		return new GenericTypeInfo<>(GenericUDAFEvaluator.AggregationBuffer.class);
	}
}
