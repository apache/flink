<?xml version="1.0" encoding="UTF-8"?>
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0"
		 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
		 xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">

	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>org.apache.flink</groupId>
		<artifactId>flink-connectors</artifactId>
		<version>1.11-SNAPSHOT</version>
		<relativePath>..</relativePath>
	</parent>

	<artifactId>flink-connector-hive_${scala.binary.version}</artifactId>
	<name>flink-connector-hive</name>

	<packaging>jar</packaging>

	<properties>
		<hiverunner.version>4.0.0</hiverunner.version>
		<reflections.version>0.9.8</reflections.version>
		<derby.version>10.10.2.0</derby.version>
	</properties>

	<dependencies>

		<!-- core dependencies -->

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-common</artifactId>
			<version>${project.version}</version>
			<scope>provided</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-hadoop-compatibility_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-api-java-bridge_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
			<scope>provided</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-runtime-blink_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
			<scope>provided</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
			<scope>provided</scope>
		</dependency>

		<!-- format dependencies -->

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-orc_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
			<exclusions>
				<exclusion>
					<groupId>org.apache.orc</groupId>
					<artifactId>orc-core</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-orc-nohive_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
			<exclusions>
				<exclusion>
					<groupId>org.apache.orc</groupId>
					<artifactId>orc-core</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-parquet_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-hadoop-fs</artifactId>
			<version>${project.version}</version>
		</dependency>

		<!-- Hadoop dependency -->
		<!-- Hadoop as provided dependencies, so we can depend on them without pulling in Hadoop -->

		<!--
			Hive 2.3.4 relies on Hadoop 2.7.2 and later versions.
			For Hadoop 2.7, the minor Hadoop version supported for flink-shaded-hadoop-2-uber is 2.7.5,
			thus override the default hadoop version from 2.4.1 to 2.7.5
		-->
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-shaded-hadoop-2-uber</artifactId>
			<version>${hivemetastore.hadoop.version}-${flink.shaded.version}</version>
			<scope>provided</scope>
		</dependency>

		<!-- Hive dependencies -->
		<!-- Note: Hive published jars do not have proper dependencies declared.
		We need to push for HIVE-16391 (https://issues.apache.org/jira/browse/HIVE-16391) to resolve this problem. -->

		<dependency>
			<groupId>org.apache.hive</groupId>
			<artifactId>hive-metastore</artifactId>
			<version>${hive.version}</version>
			<scope>provided</scope>
			<exclusions>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-shims</artifactId>
				</exclusion>
				<exclusion>
					<groupId>javolution</groupId>
					<artifactId>javolution</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.protobuf</groupId>
					<artifactId>protobuf-java</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hbase</groupId>
					<artifactId>hbase-client</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-lang</groupId>
					<artifactId>commons-lang</artifactId>
				</exclusion>
				<exclusion>
					<groupId>co.cask.tephra</groupId>
					<artifactId>tephra-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>co.cask.tephra</groupId>
					<artifactId>tephra-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>co.cask.tephra</groupId>
					<artifactId>tephra-hbase-compat-1.0</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-cli</groupId>
					<artifactId>commons-cli</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.thrift</groupId>
					<artifactId>libfb303</artifactId>
				</exclusion>
				<exclusion>
					<groupId>javax.transaction</groupId>
					<artifactId>transaction-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.orc</groupId>
					<artifactId>orc-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>joda-time</groupId>
					<artifactId>joda-time</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-1.2-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-slf4j-impl</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.ant</groupId>
					<artifactId>ant</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.tdunning</groupId>
					<artifactId>json</artifactId>
				</exclusion>
				<exclusion>
					<groupId>jline</groupId>
					<artifactId>jline</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.eclipse.jetty.aggregate</groupId>
					<artifactId>jetty-all</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.eclipse.jetty.orbit</groupId>
					<artifactId>javax.servlet</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-web</artifactId>
				</exclusion>
				<exclusion>
					<groupId>io.dropwizard.metrics</groupId>
					<artifactId>metrics-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>io.dropwizard.metrics</groupId>
					<artifactId>metrics-jvm</artifactId>
				</exclusion>
				<exclusion>
					<groupId>io.dropwizard.metrics</groupId>
					<artifactId>metrics-json</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.github.joshelser</groupId>
					<artifactId>dropwizard-metrics-hadoop-metrics2-reporter</artifactId>
				</exclusion>

				<!-- org.apache.hive:hive-service-rpc -->
				<exclusion>
					<groupId>tomcat</groupId>
					<artifactId>jasper-compiler</artifactId>
				</exclusion>
				<exclusion>
					<groupId>tomcat</groupId>
					<artifactId>jasper-runtime</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.httpcomponents</groupId>
					<artifactId>httpclient</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.httpcomponents</groupId>
					<artifactId>httpcore</artifactId>
				</exclusion>

				<!-- org.apache.hive:hive-serde -->
				<exclusion>
					<groupId>commons-codec</groupId>
					<artifactId>commons-codec</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.avro</groupId>
					<artifactId>avro</artifactId>
				</exclusion>
				<exclusion>
					<groupId>net.sf.opencsv</groupId>
					<artifactId>opencsv</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.parquet</groupId>
					<artifactId>parquet-hadoop-bundle</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.derby</groupId>
					<artifactId>derby</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.hive</groupId>
			<artifactId>hive-exec</artifactId>
			<version>${hive.version}</version>
			<scope>provided</scope>
			<exclusions>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-vector-code-gen</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-llap-tez</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-shims</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-codec</groupId>
					<artifactId>commons-codec</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-httpclient</groupId>
					<artifactId>commons-httpclient</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-slf4j-impl</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.antlr</groupId>
					<artifactId>antlr-runtime</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.antlr</groupId>
					<artifactId>ST4</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.ant</groupId>
					<artifactId>ant</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.commons</groupId>
					<artifactId>commons-compress</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.ivy</groupId>
					<artifactId>ivy</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.zookeeper</groupId>
					<artifactId>zookeeper</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.curator</groupId>
					<artifactId>apache-curator</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.curator</groupId>
					<artifactId>curator-framework</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.codehaus.groovy</groupId>
					<artifactId>groovy-all</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.calcite</groupId>
					<artifactId>calcite-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.calcite</groupId>
					<artifactId>calcite-druid</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.calcite.avatica</groupId>
					<artifactId>avatica</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.calcite</groupId>
					<artifactId>calcite-avatica</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.code.gson</groupId>
					<artifactId>gson</artifactId>
				</exclusion>
				<exclusion>
					<groupId>stax</groupId>
					<artifactId>stax-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>log4j</artifactId>
				</exclusion>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>apache-log4j-extras</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<!-- test dependencies -->

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-common</artifactId>
			<version>${project.version}</version>
			<type>test-jar</type>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-api-java</artifactId>
			<version>${project.version}</version>
			<type>test-jar</type>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-planner_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
			<type>test-jar</type>
			<scope>test</scope>
		</dependency>

		<!--flink-java and flink-clients test dependencies used for HiveInputFormatTest-->

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-java</artifactId>
			<version>${project.version}</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-clients_${scala.binary.version}</artifactId>
			<version>${project.version}</version>
			<scope>test</scope>
		</dependency>

		<dependency>
            <groupId>com.klarna</groupId>
            <artifactId>hiverunner</artifactId>
            <version>${hiverunner.version}</version>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-serde</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-jdbc</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-service</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.hive</groupId>
                    <artifactId>hive-contrib</artifactId>
                </exclusion>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-exec</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-hcatalog-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hive.hcatalog</groupId>
					<artifactId>hive-webhcat-java-client</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.tez</groupId>
					<artifactId>tez-common</artifactId>
				</exclusion>
				<exclusion>
					<!-- This dependency is no longer shipped with the JDK since Java 9.-->
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-common</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-auth</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-annotations</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-hdfs</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-mapreduce-client-core</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-api</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-client</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-common</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-server-common</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-server-web-proxy</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-shim</artifactId>
					<groupId>org.apache.tez</groupId>
				</exclusion>
				<exclusion>
					<artifactId>jms</artifactId>
					<groupId>javax.jms</groupId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
            </exclusions>
        </dependency>

		<!--  We have 0.9.10 in dependency management but hiverunner requires 0.9.8, so need to explicitly specify it here -->
		<dependency>
			<groupId>org.reflections</groupId>
			<artifactId>reflections</artifactId>
			<version>${reflections.version}</version>
			<scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-service</artifactId>
            <version>${hive.version}</version>
            <scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-exec</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-metastore</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
				<exclusion>
					<!-- This dependency is no longer shipped with the JDK since Java 9.-->
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-common</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-auth</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-client</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-annotations</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-hdfs</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-mapreduce-client-core</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-api</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-common</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-registry</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-server-applicationhistoryservice</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-server-common</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-yarn-server-resourcemanager</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hbase-hadoop-compat</artifactId>
					<groupId>org.apache.hbase</groupId>
				</exclusion>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>log4j</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
            <groupId>org.apache.hive.hcatalog</groupId>
            <artifactId>hive-hcatalog-core</artifactId>
            <version>${hive.version}</version>
            <scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-exec</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-common</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-archives</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-annotations</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-hdfs</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<artifactId>hadoop-mapreduce-client-core</artifactId>
					<groupId>org.apache.hadoop</groupId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hadoop</groupId>
					<artifactId>hadoop-yarn-server-resourcemanager</artifactId>
				</exclusion>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>log4j</artifactId>
				</exclusion>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>apache-log4j-extras</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
            </exclusions>
        </dependency>

		<dependency>
			<groupId>org.apache.hive.hcatalog</groupId>
			<artifactId>hive-webhcat-java-client</artifactId>
			<version>${hive.version}</version>
			<scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>jdk.tools</groupId>
					<artifactId>jdk.tools</artifactId>
				</exclusion>
				<exclusion>
					<artifactId>jms</artifactId>
					<groupId>javax.jms</groupId>
				</exclusion>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>log4j</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.pentaho</groupId>
					<artifactId>pentaho-aggdesigner-algorithm</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<!-- TODO: move to flink-connector-hive-test end-to-end test module once it's setup -->
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-csv</artifactId>
			<version>${project.version}</version>
			<scope>test</scope>
		</dependency>

		<dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-test-utils_${scala.binary.version}</artifactId>
            <version>${project.version}</version>
			<scope>test</scope>
        </dependency>

		<!-- The derby we use suffers some security vulnerabilities. Explicitly set it to test scope here. -->
		<dependency>
			<groupId>org.apache.derby</groupId>
			<artifactId>derby</artifactId>
			<version>${derby.version}</version>
			<scope>test</scope>
		</dependency>

    </dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-jar-plugin</artifactId>
				<executions>
					<execution>
						<goals>
							<goal>test-jar</goal>
						</goals>
					</execution>
				</executions>
			</plugin>

			<!-- Configure derby.log of embedded Hive metastore for unit tests -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-surefire-plugin</artifactId>
				<configuration>
					<forkCount>1</forkCount>
					<reuseForks>false</reuseForks>
					<systemPropertyVariables>
						<derby.stream.error.file>${project.build.directory}/derby.log</derby.stream.error.file>
					</systemPropertyVariables>
				</configuration>
			</plugin>

			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-shade-plugin</artifactId>
				<executions>
					<execution>
						<id>shade-flink</id>
						<phase>package</phase>
						<goals>
							<goal>shade</goal>
						</goals>
						<configuration>
							<shadeTestJar>false</shadeTestJar>
							<artifactSet>
								<includes combine.children="append">
									<include>org.apache.flink:flink-hadoop-fs</include>
									<include>org.apache.flink:flink-orc_${scala.binary.version}</include>
									<include>org.apache.flink:flink-orc-nohive_${scala.binary.version}</include>
									<include>org.apache.flink:flink-hadoop-compatibility_${scala.binary.version}</include>
									<include>org.apache.flink:flink-parquet_${scala.binary.version}</include>
									<include>org.apache.parquet:parquet-hadoop</include>
									<include>org.apache.parquet:parquet-format</include>
									<include>org.apache.parquet:parquet-column</include>
									<include>org.apache.parquet:parquet-common</include>
									<include>org.apache.parquet:parquet-encoding</include>
								</includes>
							</artifactSet>
							<relocations>
								<relocation>
									<pattern>org.apache.flink.runtime.fs.hdfs</pattern>
									<shadedPattern>org.apache.flink.connectors.hive.fs.hdfs</shadedPattern>
								</relocation>
								<relocation>
									<pattern>org.apache.parquet</pattern>
									<shadedPattern>org.apache.flink.shaded.org.apache.parquet</shadedPattern>
								</relocation>
								<relocation>
									<pattern>shaded.parquet</pattern>
									<shadedPattern>org.apache.flink.reshaded.parquet</shadedPattern>
								</relocation>
							</relocations>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>
	</build>

	<profiles>
		<!-- Activate these profiles with -Phive-x.x.x to build and test against different Hive versions -->
		<profile>
			<id>hive-1.0.1</id>
			<properties>
				<hive.version>1.0.1</hive.version>
				<hivemetastore.hadoop.version>2.6.5</hivemetastore.hadoop.version>
				<hiverunner.version>3.1.1</hiverunner.version>
				<derby.version>10.10.1.1</derby.version>
			</properties>
			<dependencies>
				<dependency>
					<groupId>org.apache.orc</groupId>
					<artifactId>orc-core</artifactId>
					<version>${orc.version}</version>
					<classifier>nohive</classifier>
					<exclusions>
						<exclusion>
							<groupId>org.apache.hadoop</groupId>
							<artifactId>hadoop-common</artifactId>
						</exclusion>
						<exclusion>
							<groupId>org.apache.hive</groupId>
							<artifactId>hive-storage-api</artifactId>
						</exclusion>
					</exclusions>
				</dependency>
			</dependencies>
		</profile>
		<profile>
			<!-- Hive-1.1.1 has issue that it can't handle 'stored as file_format' syntax. So test against 1.1.0 -->
			<id>hive-1.1.0</id>
			<properties>
				<hive.version>1.1.0</hive.version>
				<hivemetastore.hadoop.version>2.6.5</hivemetastore.hadoop.version>
				<hiverunner.version>3.1.1</hiverunner.version>
				<derby.version>10.11.1.1</derby.version>
			</properties>
			<dependencies>
				<dependency>
					<groupId>org.apache.orc</groupId>
					<artifactId>orc-core</artifactId>
					<version>${orc.version}</version>
					<classifier>nohive</classifier>
					<exclusions>
						<exclusion>
							<groupId>org.apache.hadoop</groupId>
							<artifactId>hadoop-common</artifactId>
						</exclusion>
						<exclusion>
							<groupId>org.apache.hive</groupId>
							<artifactId>hive-storage-api</artifactId>
						</exclusion>
					</exclusions>
				</dependency>
			</dependencies>
		</profile>
		<profile>
			<id>hive-1.2.1</id>
			<properties>
				<hive.version>1.2.1</hive.version>
				<hivemetastore.hadoop.version>2.6.5</hivemetastore.hadoop.version>
				<hiverunner.version>3.2.1</hiverunner.version>
			</properties>
			<dependencies>
				<dependency>
					<groupId>org.apache.orc</groupId>
					<artifactId>orc-core</artifactId>
					<version>${orc.version}</version>
					<classifier>nohive</classifier>
					<exclusions>
						<exclusion>
							<groupId>org.apache.hadoop</groupId>
							<artifactId>hadoop-common</artifactId>
						</exclusion>
						<exclusion>
							<groupId>org.apache.hive</groupId>
							<artifactId>hive-storage-api</artifactId>
						</exclusion>
					</exclusions>
				</dependency>
			</dependencies>
		</profile>
		<profile>
			<id>hive-2.0.0</id>
			<properties>
				<hive.version>2.0.0</hive.version>
			</properties>
		</profile>
		<profile>
			<id>hive-2.1.1</id>
			<properties>
				<hive.version>2.1.1</hive.version>
			</properties>
		</profile>
		<profile>
			<id>hive-2.2.0</id>
			<properties>
				<hive.version>2.2.0</hive.version>
			</properties>
			<dependencies>
				<dependency>
					<groupId>org.apache.orc</groupId>
					<artifactId>orc-core</artifactId>
					<version>${orc.version}</version>
					<exclusions>
						<exclusion>
							<groupId>org.apache.hadoop</groupId>
							<artifactId>hadoop-common</artifactId>
						</exclusion>
					</exclusions>
				</dependency>
			</dependencies>
		</profile>
		<profile>
			<id>hive-3.1.1</id>
			<properties>
				<hive.version>3.1.1</hive.version>
				<derby.version>10.14.1.0</derby.version>
			</properties>

		</profile>
		<profile>
			<id>skip-hive-tests</id>
			<build>
				<plugins>
					<plugin>
						<groupId>org.apache.maven.plugins</groupId>
						<artifactId>maven-surefire-plugin</artifactId>
						<configuration>
							<skipTests>true</skipTests>
						</configuration>
					</plugin>
				</plugins>
			</build>
		</profile>
		<profile>
			<id>java11</id>
			<activation>
				<jdk>11</jdk>
			</activation>

			<build>
				<plugins>
					<plugin>
						<groupId>org.apache.maven.plugins</groupId>
						<artifactId>maven-surefire-plugin</artifactId>
						<configuration>
							<!-- hive does not support Java 11 -->
							<skip>true</skip>
						</configuration>
					</plugin>
				</plugins>
			</build>
		</profile>
	</profiles>
</project>
