# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: "Reusable workflow example"

on:
  workflow_call:
    inputs:
      test_pool_name:
        description: "defines the hardware pool for compilation and unit test execution."
        required: true
        type: string
      e2e_pool_definion:
        description: "defines the hardware pool for end-to-end test execution"
        required: true
        type: string
      stage_name:
        description: "defines a unique identifier for all jobs in a stage (in case the jobs are added multiple times to a stage)"
        required: true
        type: string
      environment:
        description: "defines environment variables for downstream scripts"
        required: true
        type: string
      run_end_to_end:
        description: "if set to 'true', the end to end tests will be executed"
        required: true
        type: boolean
      jdk:
        description: "the jdk version to use"
        required: true
        type: number
      mode:
        description: "mode of execution. Can be nightly, e2e, release"
        required: true
        type: string
      cache_key:
        description: "key for the Maven cache"
        required: true
        type: string
      cache_fallback_key:
        description: "fallback key for the Maven cache"
        required: true
        type: string
      maven_cache_folder:
        description: "folder for the Maven cache"
        required: true
        type: string
      flink_artifact_dir:
        description: "Directory for the build artifacts"
        required: true
        type: string
      docker_images_cache_key:
        description: "key for the Docker images cache"
        required: true
        type: string
      docker_images_cache_folder:
        description: "folder for the Docker images cache"
        required: true
        type: string
#    secrets:
#      token:
#        required: false

jobs:
  compile:
    name: "compile_${{ inputs.stage_name }}"
    runs-on: ubuntu-latest #${{parameters.test_pool_definition}}
    if: success() && (inputs.mode != e2e)
    timeout-minutes: 240 #cancelTimeoutInMinutes: 1 ???
    steps:
      - name: "TODO: AutoModality/action-clean@v1 Should only be necessary if specifying runs-on: self-hosted"
      - name: "TODO: free up disk space required? Original version called ./tools/azure-pipelines/free_disk_space.sh"
      - name: "Set up Maven" #and SSL??? This replaces the container from the original yml
        uses: stCarolas/setup-maven@v4.2
        with:
          maven-version: 3.2.5
      # The cache task is persisting the .m2 directory between builds, so that
      # we do not have to re-download all dependencies from maven central for
      # each build. The hope is that downloading the cache is faster than
      # all dependencies individually.
      # In this configuration, we use a hash over all committed (not generated) .pom files
      # as a key for the build cache (CACHE_KEY). If we have a cache miss on the hash
      # (usually because a pom file has changed), we'll fall back to a key without
      # the pom files (CACHE_FALLBACK_KEY).
      - name: "Cache Maven local repo"
        if: inputs.test_pool_name != "Default" # This probably doesn't work as input cannot be an object
        uses: actions/cache@v2
        with:
          path: ${{ inputs.maven_cache_folder }}
          key: ${{ inputs.cache_key }}
          restore-keys: ${{ inputs.cache_fallback_key }}
      - name: "Set JDK"
        run: |
          echo "::set-output name=JAVA_HOME::${JAVA_HOME}_${{ inputs.jdk }}_X64"
          echo "::set-output name=PATH::${JAVA_HOME}_${{ inputs.jdk }}_X64/bin:$PATH"
      # Compile
      - name: "Compile"
        run: |
          ${{ inputs.environment }} ./tools/ci/compile.sh || exit $?
          ./tools/azure-pipelines/create_build_artifact.sh
      # upload artifacts for next stage
      - name: Upload artifacts
        uses: actions/upload-artifact@v2
        with:
          name: FlinkCompileArtifact-${{ inputs.stage_name }}
          path: ${{ inputs.flink_artifact_dir }}
  test:
    name: "test_${{ inputs.stage_name }}"
    needs: compile
    runs-on: ubuntu-20.04
    if: success() && inputs.mode != "e2e"
    timeout-minutes: 240 #cancelTimeoutInMinutes: 1 ???
    strategy:
      matrix:
        module:
          - core
          - python
          - libraries
          - table
          - connectors
          - kafka/gelly
          - tests
          - misc
          - finegrained_resource_management

    steps:
      - name: "TODO: AutoModality/action-clean@v1 Should only be necessary if specifying runs-on: self-hosted"
      - name: "TODO: free up disk space required? Original version called ./tools/azure-pipelines/free_disk_space.sh"
      - name: "Set up Maven" #and SSL??? This replaces the container from the original yml
        uses: stCarolas/setup-maven@v4.2
        with:
          maven-version: 3.2.5
      # download artifact from compile stage
      - name: "Download pipeline artifact"
        uses: actions/download-artifact@v2
        with:
          name: FlinkCompileArtifact-${{ inputs.stage_name }}
          path: ${{ inputs.flink_artifact_dir }}
      - name: "Unpack Build artifact"
        run: FLINK_ARTIFACT_DIR=${{ inputs.flink_artifact_dir }} ./tools/azure-pipelines/unpack_build_artifact.sh
      - name: "Cache Maven local repo"
        if: inputs.test_pool_name != "Default" # This probably doesn't work as input cannot be an object
        uses: actions/cache@v2
        with:
          path: ${{ inputs.maven_cache_folder }}
          key: ${{ inputs.cache_key }}
          restore-keys: ${{ inputs.cache_fallback_key }}
      - name: "Cache Docker images"
        id: docker-cache
        if: inputs.test_pool_name != "Default" # This probably doesn't work as input cannot be an object
        uses: actions/cache@v2
        with:
          path: ${{ inputs.docker_images_cache_folder }}
          key: "${{ matrix.module }}-${{ inputs.docker_images_cache_folder }}"
          restore-keys: ${{ inputs.cache_fallback_key }}
      - name: "Load Docker images if not present in the cache, yet"
        if: cancelled() && !steps.docker-cache.cache.hit
        run: ./tools/azure-pipelines/cache_docker_images.sh load

jobs:
  - job: test_${{parameters.stage_name}}
    steps:
      - script: |
          echo "##vso[task.setvariable variable=JAVA_HOME]$JAVA_HOME_${{parameters.jdk}}_X64"
          echo "##vso[task.setvariable variable=PATH]$JAVA_HOME_${{parameters.jdk}}_X64/bin:$PATH"
        displayName: "Set JDK"

      - script: sudo sysctl -w kernel.core_pattern=core.%p
        displayName: Set coredump pattern

      # Test
      - script: ${{parameters.environment}} ./tools/azure-pipelines/uploading_watchdog.sh ./tools/ci/test_controller.sh $(module)
        displayName: Test - $(module)
        env:
          IT_CASE_S3_BUCKET: $(SECRET_S3_BUCKET)
          IT_CASE_S3_ACCESS_KEY: $(SECRET_S3_ACCESS_KEY)
          IT_CASE_S3_SECRET_KEY: $(SECRET_S3_SECRET_KEY)
          IT_CASE_GLUE_SCHEMA_ACCESS_KEY: $(SECRET_GLUE_SCHEMA_ACCESS_KEY)
          IT_CASE_GLUE_SCHEMA_SECRET_KEY: $(SECRET_GLUE_SCHEMA_SECRET_KEY)

      - task: PublishTestResults@2
        condition: succeededOrFailed()
        inputs:
          testResultsFormat: 'JUnit'

      # upload debug artifacts
      - task: PublishPipelineArtifact@1
        condition: not(eq('$(DEBUG_FILES_OUTPUT_DIR)', ''))
        displayName: Upload Logs
        inputs:
          targetPath: $(DEBUG_FILES_OUTPUT_DIR)
          artifact: logs-${{parameters.stage_name}}-$(DEBUG_FILES_NAME)

      - script: ./tools/azure-pipelines/cache_docker_images.sh save
        displayName: Save docker images
        condition: and(not(canceled()), or(failed(), ne(variables.DOCKER_IMAGES_CACHE_HIT, 'true')))
        continueOnError: true

  - template: e2e-template.yml
    parameters:
      stage_name: ${{parameters.stage_name}}
      e2e_pool_definition: ${{parameters.e2e_pool_definition}}
      environment: ${{parameters.environment}}
      jdk: ${{parameters.jdk}}
      group: 1
  - template: e2e-template.yml
    parameters:
      stage_name: ${{parameters.stage_name}}
      e2e_pool_definition: ${{parameters.e2e_pool_definition}}
      environment: ${{parameters.environment}}
      jdk: ${{parameters.jdk}}
      group: 2
