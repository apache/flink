<table class="configuration table table-bordered">
    <thead>
        <tr>
            <th class="text-left" style="width: 20%">Key</th>
            <th class="text-left" style="width: 15%">Default</th>
            <th class="text-left" style="width: 10%">Type</th>
            <th class="text-left" style="width: 55%">Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><h5>jobmanager.adaptive-scheduler.min-parallelism-increase</h5></td>
            <td style="word-wrap: break-word;">1</td>
            <td>Integer</td>
            <td>Configure the minimum increase in parallelism for a job to scale up.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.adaptive-scheduler.resource-stabilization-timeout</h5></td>
            <td style="word-wrap: break-word;">10 s</td>
            <td>Duration</td>
            <td>The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.<br />If <code class="highlighter-rouge">scheduler-mode</code> is configured to <code class="highlighter-rouge">REACTIVE</code>, this configuration value will default to 0, so that jobs are starting immediately with the available resources.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.adaptive-scheduler.resource-wait-timeout</h5></td>
            <td style="word-wrap: break-word;">5 min</td>
            <td>Duration</td>
            <td>The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.<br />Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).<br />Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.<br />If <code class="highlighter-rouge">scheduler-mode</code> is configured to <code class="highlighter-rouge">REACTIVE</code>, this configuration value will default to a negative value to disable the resource timeout.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.adaptive-scheduler.scaling-interval.max</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Duration</td>
            <td>Determines the maximum interval time after which a scaling operation is forced even if the <code class="highlighter-rouge">jobmanager.adaptive-scheduler.min-parallelism-increase</code> aren't met. The scaling operation will be ignored when the resource hasn't changed. This option is disabled by default.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.adaptive-scheduler.scaling-interval.min</h5></td>
            <td style="word-wrap: break-word;">30 s</td>
            <td>Duration</td>
            <td>Determines the minimum time between scaling operations.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.archive.fs.dir</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>Directory for JobManager to store the archives of completed jobs.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.bind-host</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>The local address of the network interface that the job manager binds to. If not configured, '0.0.0.0' will be used.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.execution.attempts-history-size</h5></td>
            <td style="word-wrap: break-word;">16</td>
            <td>Integer</td>
            <td>The maximum number of historical execution attempts kept in history.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.execution.failover-strategy</h5></td>
            <td style="word-wrap: break-word;">"region"</td>
            <td>String</td>
            <td>This option specifies how the job computation recovers from task failures. Accepted values are:<ul><li>'full': Restarts all tasks to recover the job.</li><li>'region': Restarts all tasks that could be affected by the task failure. More details can be found <a href="{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/ops/state/task_failure_recovery/#restart-pipelined-region-failover-strategy">here</a>.</li></ul></td>
        </tr>
        <tr>
            <td><h5>jobmanager.failure-enrichers</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>An optional list of failure enricher names. If empty, NO failure enrichers will be started. If configured, only enrichers whose name matches any of the names in the list will be started.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.future-pool.size</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.io-pool.size</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.partition.hybrid.partition-data-consume-constraint</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td><p>Enum</p></td>
            <td>Controls the constraint that hybrid partition data can be consumed. Note that this option is allowed only when <code class="highlighter-rouge">jobmanager.scheduler</code> has been set to <code class="highlighter-rouge">AdaptiveBatch</code>. Accepted values are:<ul><li>'<code class="highlighter-rouge">ALL_PRODUCERS_FINISHED</code>': hybrid partition data can be consumed only when all producers are finished.</li><li>'<code class="highlighter-rouge">ONLY_FINISHED_PRODUCERS</code>': hybrid partition data can be consumed when its producer is finished.</li><li>'<code class="highlighter-rouge">UNFINISHED_PRODUCERS</code>': hybrid partition data can be consumed even if its producer is un-finished.</li></ul><br /><br />Possible values:<ul><li>"ALL_PRODUCERS_FINISHED"</li><li>"ONLY_FINISHED_PRODUCERS"</li><li>"UNFINISHED_PRODUCERS"</li></ul></td>
        </tr>
        <tr>
            <td><h5>jobmanager.resource-id</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>The JobManager's ResourceID. If not configured, the ResourceID will be generated randomly.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.retrieve-taskmanager-hostname</h5></td>
            <td style="word-wrap: break-word;">true</td>
            <td>Boolean</td>
            <td>Flag indicating whether JobManager would retrieve canonical host name of TaskManager during registration. If the option is set to "false", TaskManager registration with JobManager could be faster, since no reverse DNS lookup is performed. However, local input split assignment (such as for HDFS files) may be impacted.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.rpc.address</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.rpc.bind-port</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>The local RPC port that the JobManager binds to. If not configured, the external port (configured by 'jobmanager.rpc.port') will be used.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.rpc.port</h5></td>
            <td style="word-wrap: break-word;">6123</td>
            <td>Integer</td>
            <td>The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.</td>
        </tr>
        <tr>
            <td><h5>jobmanager.scheduler</h5></td>
            <td style="word-wrap: break-word;">Default</td>
            <td><p>Enum</p></td>
            <td>Determines which scheduler implementation is used to schedule tasks. Accepted values are:<ul><li>'Default': Default scheduler</li><li>'Adaptive': Adaptive scheduler. More details can be found <a href="{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/deployment/elastic_scaling#adaptive-scheduler">here</a>.</li><li>'AdaptiveBatch': Adaptive batch scheduler. More details can be found <a href="{{.Site.BaseURL}}{{.Site.LanguagePrefix}}/docs/deployment/elastic_scaling#adaptive-batch-scheduler">here</a>.</li></ul><br /><br />Possible values:<ul><li>"Default"</li><li>"Adaptive"</li><li>"AdaptiveBatch"</li></ul></td>
        </tr>
        <tr>
            <td><h5>jobstore.cache-size</h5></td>
            <td style="word-wrap: break-word;">52428800</td>
            <td>Long</td>
            <td>The job store cache size in bytes which is used to keep completed jobs in memory.</td>
        </tr>
        <tr>
            <td><h5>jobstore.expiration-time</h5></td>
            <td style="word-wrap: break-word;">3600</td>
            <td>Long</td>
            <td>The time in seconds after which a completed job expires and is purged from the job store.</td>
        </tr>
        <tr>
            <td><h5>jobstore.max-capacity</h5></td>
            <td style="word-wrap: break-word;">infinite</td>
            <td>Integer</td>
            <td>The max number of completed jobs that can be kept in the job store. NOTICE: if memory store keeps too many jobs in session cluster, it may cause FullGC or OOM in jm.</td>
        </tr>
        <tr>
            <td><h5>jobstore.type</h5></td>
            <td style="word-wrap: break-word;">File</td>
            <td><p>Enum</p></td>
            <td>Determines which job store implementation is used in session cluster. Accepted values are:<ul><li>'File': the file job store keeps the archived execution graphs in files</li><li>'Memory': the memory job store keeps the archived execution graphs in memory. You may need to limit the <code class="highlighter-rouge">jobstore.max-capacity</code> to mitigate FullGC or OOM when there are too many graphs</li></ul><br /><br />Possible values:<ul><li>"File"</li><li>"Memory"</li></ul></td>
        </tr>
        <tr>
            <td><h5>web.exception-history-size</h5></td>
            <td style="word-wrap: break-word;">16</td>
            <td>Integer</td>
            <td>The maximum number of failures collected by the exception history per job.</td>
        </tr>
    </tbody>
</table>
