<table class="configuration table table-bordered">
    <thead>
        <tr>
            <th class="text-left" style="width: 20%">Key</th>
            <th class="text-left" style="width: 15%">Default</th>
            <th class="text-left" style="width: 10%">Type</th>
            <th class="text-left" style="width: 55%">Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><h5>taskmanager.network.compression.codec</h5></td>
            <td style="word-wrap: break-word;">LZ4</td>
            <td><p>Enum</p></td>
            <td>The codec to be used when compressing shuffle data. If it is "NONE", compression is disable. If it is not "NONE", only "LZ4", "LZO" and "ZSTD" are supported now. Through tpc-ds test of these three algorithms, the results show that "LZ4" algorithm has the highest compression and decompression speed, but the compression ratio is the lowest. "ZSTD" has the highest compression ratio, but the compression and decompression speed is the slowest, and LZO is between the two. Also note that this option is experimental and might be changed in the future.<br /><br />Possible values:<ul><li>"NONE"</li><li>"LZ4"</li><li>"LZO"</li><li>"ZSTD"</li></ul></td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.detailed-metrics</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>Boolean flag to enable/disable more detailed metrics about inbound/outbound network queue lengths.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.hybrid-shuffle.external-remote-tier-factory.class</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>The option configures the class that is responsible for creating an external remote tier factory for hybrid shuffle. If configured, the hybrid shuffle will only initialize the specified remote tier according to the given class name. Currently, since the tier interfaces are not yet public and are still actively evolving, it is recommended that users do not independently implement the external remote tier until the tier interfaces are stabilized. </td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.hybrid-shuffle.remote.path</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>String</td>
            <td>The option is used to configure the base path of remote storage for hybrid shuffle. The shuffle data will be stored in remote storage when the disk space is not enough. Note: If the option is configured and taskmanager.network.hybrid-shuffle.enable-new-mode is false, this option will be ignored. If the option is not configured and taskmanager.network.hybrid-shuffle.enable-new-mode is true, the remote storage will be disabled.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.memory.buffer-debloat.enabled</h5></td>
            <td style="word-wrap: break-word;">false</td>
            <td>Boolean</td>
            <td>The switch of the automatic buffered debloating feature. If enabled the amount of in-flight data will be adjusted automatically accordingly to the measured throughput.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.memory.buffer-debloat.period</h5></td>
            <td style="word-wrap: break-word;">200 ms</td>
            <td>Duration</td>
            <td>The minimum period of time after which the buffer size will be debloated if required. The low value provides a fast reaction to the load fluctuation but can influence the performance.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.memory.buffer-debloat.samples</h5></td>
            <td style="word-wrap: break-word;">20</td>
            <td>Integer</td>
            <td>The number of the last buffer size values that will be taken for the correct calculation of the new one.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.memory.buffer-debloat.target</h5></td>
            <td style="word-wrap: break-word;">1 s</td>
            <td>Duration</td>
            <td>The target total time after which buffered in-flight data should be fully consumed. This configuration option will be used, in combination with the measured throughput, to adjust the amount of in-flight data.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.memory.buffer-debloat.threshold-percentages</h5></td>
            <td style="word-wrap: break-word;">25</td>
            <td>Integer</td>
            <td>The minimum difference in percentage between the newly calculated buffer size and the old one to announce the new value. Can be used to avoid constant back and forth small adjustments.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.memory.read-buffer.required-per-gate.max</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>The maximum number of network read buffers that are required by an input gate. (An input gate is responsible for reading data from all subtasks of an upstream task.) The number of buffers needed by an input gate is dynamically calculated in runtime, depending on various factors (e.g., the parallelism of the upstream task). Among the calculated number of needed buffers, the part below this configured value is required, while the excess part, if any, is optional. A task will fail if the required buffers cannot be obtained in runtime. A task will not fail due to not obtaining optional buffers, but may suffer a performance reduction. If not explicitly configured, the default value is Integer.MAX_VALUE for streaming workloads, and 1000 for batch workloads. If explicitly configured, the configured value should be at least 1.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.netty.client.connectTimeoutSec</h5></td>
            <td style="word-wrap: break-word;">120</td>
            <td>Integer</td>
            <td>The Netty client connection timeout.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.netty.client.tcp.keepCount</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>The maximum number of keepalive probes TCP should send before Netty client dropping the connection. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.netty.client.tcp.keepIdleSec</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>The time (in seconds) the connection needs to remain idle before TCP starts sending keepalive probes. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.netty.client.tcp.keepIntervalSec</h5></td>
            <td style="word-wrap: break-word;">(none)</td>
            <td>Integer</td>
            <td>The time (in seconds) between individual keepalive probes. Note: This will not take effect when using netty transport type of nio with an older version of JDK 8, refer to https://bugs.openjdk.org/browse/JDK-8194298.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.partition-request-timeout</h5></td>
            <td style="word-wrap: break-word;">10 s</td>
            <td>Duration</td>
            <td>Timeout for an individual partition request of remote input channels. The partition request will finally fail if the total wait time exceeds twice the value of <code class="highlighter-rouge">taskmanager.network.request-backoff.max</code>.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.request-backoff.initial</h5></td>
            <td style="word-wrap: break-word;">100</td>
            <td>Integer</td>
            <td>Minimum backoff in milliseconds for partition requests of local input channels.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.request-backoff.max</h5></td>
            <td style="word-wrap: break-word;">10000</td>
            <td>Integer</td>
            <td>Maximum backoff in milliseconds for partition requests of local input channels.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.retries</h5></td>
            <td style="word-wrap: break-word;">0</td>
            <td>Integer</td>
            <td>The number of retry attempts for network communication. Currently it's only used for establishing input/output channel connections</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.sort-shuffle.min-buffers</h5></td>
            <td style="word-wrap: break-word;">512</td>
            <td>Integer</td>
            <td>Minimum number of network buffers required per blocking result partition for sort-shuffle. For production usage, it is suggested to increase this config value to at least 2048 (64M memory if the default 32K memory segment size is used) to improve the data compression ratio and reduce the small network packets. Usually, several hundreds of megabytes memory is enough for large scale batch jobs. Note: you may also need to increase the size of total network memory to avoid the 'insufficient number of network buffers' error if you are increasing this config value.</td>
        </tr>
        <tr>
            <td><h5>taskmanager.network.tcp-connection.enable-reuse-across-jobs</h5></td>
            <td style="word-wrap: break-word;">true</td>
            <td>Boolean</td>
            <td>Whether to reuse tcp connections across multi jobs. If set to true, tcp connections will not be released after job finishes. The subsequent jobs will be free from the overhead of the connection re-establish. However, this may lead to an increase in the total number of connections on your machine. When it reaches the upper limit, you can set it to false to release idle connections. Note that to avoid connection leak, you must set taskmanager.network.max-num-tcp-connections to a smaller value before you enable tcp connection reuse.</td>
        </tr>
    </tbody>
</table>
